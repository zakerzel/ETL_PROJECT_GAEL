# ğŸŒ ETL Airflow Project â€” Natural Events Monitoring

This project implements a **batch ETL pipeline** using **Apache Airflow**, **MongoDB**, and **Streamlit**, designed to ingest and visualize real-world environmental data.

## ğŸ§  Objective

To build a fully dockerized data pipeline that:

- Ingests data from **three public APIs**
- Transforms and enriches the data
- Stores raw and processed versions in MongoDB
- Displays the processed results through an interactive **Streamlit dashboard**

---

## ğŸŒ Public APIs Used

| Source | API | Link |
|--------|-----|------|
| ğŸŒ¤ï¸ Weather | Open-Meteo | [open-meteo.com](https://open-meteo.com/en/docs) |
| ğŸŒ Earthquakes | USGS Earthquake API | [earthquake.usgs.gov](https://earthquake.usgs.gov/earthquakes/feed/v1.0/geojson.php) |
| ğŸ”¥ Natural Events | NASA EONET API | [eonet.gsfc.nasa.gov](https://eonet.gsfc.nasa.gov/docs/v3) |

---

## ğŸ—ƒï¸ MongoDB Collections

| Type | Collection | Purpose |
|------|------------|---------|
| Raw  | `raw_weather`<br>`raw_quakes`<br>`raw_events` | Stores unprocessed API data |
| Processed | `processed_weather`<br>`processed_quakes`<br>`processed_events` | Cleaned, enriched data for dashboard |

---

## âš™ï¸ How to Run

### 1. Clone the project

```bash
git clone https://github.com/youruser/your-etl-project.git
cd your-etl-project
2. Start all services with Docker
bash
Copiar
Editar
docker-compose up --build
This starts:

Airflow (webserver + scheduler)

PostgreSQL (metadata)

MongoDB (data storage)

Streamlit dashboard

ğŸ› ï¸ How to Use
ğŸ”„ Run the ETL DAG
Visit Airflow UI at http://localhost:8080

DAG name: etl_natural_events_pipeline

Toggle it on (ğŸŸ¢)

Click "Trigger DAG" to execute manually

ğŸ“„ Check logs
Go to the DAG > Task Instances

Click any task > "View Log"

ğŸ“Š Streamlit Dashboard
Visit http://localhost:8501

You can:

View current weather in MÃ©rida, YucatÃ¡n

Browse recent earthquakes

Filter natural events by category (e.g., wildfires, volcanoes)

ğŸ” XCom Usage
Airflow XComs are used to pass data between tasks:

Each ingest_*.py task pushes raw API data to XCom

transform_all.py pulls from XCom, cleans data, and pushes transformed records

load_mongo.py pulls transformed records from XCom and inserts them into MongoDB

ğŸ“‚ Folder Structure
Copiar
Editar
project-root/
â”œâ”€â”€ dags/
â”‚   â”œâ”€â”€ main_pipeline.py
â”‚   â”œâ”€â”€ ingestion_weather.py
â”‚   â”œâ”€â”€ ingestion_quakes.py
â”‚   â”œâ”€â”€ ingestion_events.py
â”‚   â”œâ”€â”€ transform_all.py
â”‚   â”œâ”€â”€ load_mongo.py
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ mongo_utils.py
â”‚       â”œâ”€â”€ api_helpers.py
â”‚       â””â”€â”€ transform_helpers.py
â”‚
â”œâ”€â”€ streamlit_app/
â”‚   â”œâ”€â”€ app.py
â”‚   â””â”€â”€ Dockerfile
â”‚
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
âœ… Project Checklist
 ETL DAG with 5+ tasks

 3 public APIs

 MongoDB for raw + processed data

 Streamlit dashboard

 Dockerized solution

 XComs between tasks

Developed as part of the Massive Data Management course at Universidad PolitÃ©cnica de YucatÃ¡n.